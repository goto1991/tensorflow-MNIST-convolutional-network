{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need more than 0 values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1b663c717c3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Greys_r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Anaconda/lib/python2.7/site-packages/matplotlib/axes/_base.pyc\u001b[0m in \u001b[0;36mset_xlim\u001b[0;34m(self, left, right, emit, auto, **kw)\u001b[0m\n\u001b[1;32m   2762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mright\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2764\u001b[0;31m             \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2766\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_unit_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need more than 0 values to unpack"
     ]
    }
   ],
   "source": [
    "import cPickle as cp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sci\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(16):\n",
    "    ax = fig.add_subplot(4, 4, i + 1)\n",
    "    ax.set_xlim(())\n",
    "    ax.set_ylim(())\n",
    "    ax.imshow(mnist.training.images[i].reshape(28, 28), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"%matplotlib notebook\\n\",\n",
    "    \"\\n\",\n",
    "    \"from matplotlib import pylab\\n\",\n",
    "    \"pylab.rcParams['figure.figsize'] = (10.0, 10.0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"from tensorflow.examples.tutorials.mnist import input_data\\n\",\n",
    "    \"\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import tensorflow as tf\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Initialisations\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let us write some helper functions to initialise weights and biases. We'll initialise weights as Gaussian random variables with mean 0 and variance 0.0025. For biases we'll initialise everything with a constant 0.1. This is because we're mainly going to be using ReLU non-linearities.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def weight_variable(shape):\\n\",\n",
    "    \"    initial = tf.truncated_normal(shape, stddev=0.05)\\n\",\n",
    "    \"    return tf.Variable(initial)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def bias_variable(shape):\\n\",\n",
    "    \"    initial = tf.constant(0.1, shape=shape)\\n\",\n",
    "    \"    return tf.Variable(initial)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Model\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's define the model. The model is defined as follows:\\n\",\n",
    "    \"\\n\",\n",
    "    \"* An input that is 728 dimensional vector. \\n\",\n",
    "    \"* Reshape the input as 28x28x1 images (only 1 because they are grey scale) \\n\",\n",
    "    \"* A convolutional layer with 25 filters of shape 12x12x1 and a ReLU non-linearity (with stride (2, 2) and no padding)\\n\",\n",
    "    \"* A convolutional layer with 64 filters of shape 5x5x25 and a ReLU non-linearity (with stride (1, 2) and padding to maintain size)\\n\",\n",
    "    \"* A max_pooling layer of shape 2x2\\n\",\n",
    "    \"* A fully connected layer taking all the outputs of the max_pooling layer to 1024 units and ReLU nonlinearity\\n\",\n",
    "    \"* A fully connected layer taking 1024 units to 10 no activation function (the softmax non-linearity will be included in the loss function rather than in the model)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": false\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"x = tf.placeholder(tf.float32, shape=[None, 784])\\n\",\n",
    "    \"x_ = tf.reshape(x, [-1, 28, 28, 1])\\n\",\n",
    "    \"y_ = tf.placeholder(tf.float32, shape=[None, 10])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define the first convolution layer here\\n\",\n",
    "    \"# TODO\\n\",\n",
    "    \"# W_conv1 =\\n\",\n",
    "    \"# b_conv1 = \\n\",\n",
    "    \"# h_conv1 = \\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define the second convolution layer here\\n\",\n",
    "    \"# W_conv2 = \\n\",\n",
    "    \"# b_conv2 = \\n\",\n",
    "    \"# h_conv2 = \\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define maxpooling\\n\",\n",
    "    \"# h_pool2 = \\n\",\n",
    "    \"\\n\",\n",
    "    \"# All subsequent layers will be fully connected ignoring geometry so we'll flatten the layer\\n\",\n",
    "    \"# Flatten the h_pool2_layer (as it has a multidimensiona shape) \\n\",\n",
    "    \"# h_pool2_flat = \\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define the first fully connected layer here\\n\",\n",
    "    \"# W_fc1 = \\n\",\n",
    "    \"# b_fc1 = \\n\",\n",
    "    \"# h_fc1 = \\n\",\n",
    "    \"\\n\",\n",
    "    \"# Use dropout for this layer (should you wish)\\n\",\n",
    "    \"# keep_prob = tf.placeholder(tf.float32)\\n\",\n",
    "    \"# h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# The final fully connected layer\\n\",\n",
    "    \"# W_fc2 = \\n\",\n",
    "    \"# b_fc2 = \\n\",\n",
    "    \"# y_conv = \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### Loss Function, Accuracy and Training Algorithm\\n\",\n",
    "    \"\\n\",\n",
    "    \"* We'll use the cross entropy loss function. The loss function is called `tf.nn.cross_entropy_with_logits` in tensorflow\\n\",\n",
    "    \"\\n\",\n",
    "    \"* Accuray is simply defined as the fraction of data correctly classified\\n\",\n",
    "    \"\\n\",\n",
    "    \"* For training you should use the AdamOptimizer (read the documentation) and set the learning rate to be 1e-4. You are welcome, and in fact encouraged, to experiment with other optimisation procedures and learning rates. \\n\",\n",
    "    \"\\n\",\n",
    "    \"* (Optional): You may even want to use different filter sizes once you are finished with experimenting with what is asked in this practial\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": false\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# We'll use the cross entropy loss function \\n\",\n",
    "    \"cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# And classification accuracy\\n\",\n",
    "    \"correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\\n\",\n",
    "    \"accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# And the Adam optimiser\\n\",\n",
    "    \"train_step = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cross_entropy)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": false\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load the mnist data\\n\",\n",
    "    \"mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": false\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Let us visualise the first 16 data points from the MNIST training data\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig = plt.figure()\\n\",\n",
    "    \"for i in range(16):\\n\",\n",
    "    \"    ax = fig.add_subplot(4, 4, i + 1)\\n\",\n",
    "    \"    ax.set_xticks(())\\n\",\n",
    "    \"    ax.set_yticks(())\\n\",\n",
    "    \"    ax.imshow(mnist.train.images[i].reshape(28, 28), cmap='Greys_r')  \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": false\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Start a tf session and run the optimisation algorithm\\n\",\n",
    "    \"sess = tf.Session()\\n\",\n",
    "    \"sess.run(tf.initialize_all_variables())\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i in range(3000):\\n\",\n",
    "    \"    batch = mnist.train.next_batch(50)\\n\",\n",
    "    \"    #TODO\\n\",\n",
    "    \"    # Write the optimisation code here\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": false\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Print accuracy on the test set\\n\",\n",
    "    \"# print ('Test accuracy: %g' % sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### Visualising the Filters\\n\",\n",
    "    \"\\n\",\n",
    "    \"We'll now visualise all the 32 filters in the first convolution layer. As they are each of shape 12x12x1, they may themselves be viewed as greyscale images. Visualising filters in further layers is more complicated and involves modifying the neural network. See the [paper](http://www.matthewzeiler.com/pubs/arxive2013/arxive2013.pdf) by Matt Zeiler and Rob Fergus if you are interested. \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": false\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Visualise the filters in the first convolutional layer\\n\",\n",
    "    \"with sess.as_default():\\n\",\n",
    "    \"    W = W_conv1.eval()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add code to visualise fitlers here\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### Identifying image patches that activate the filters\\n\",\n",
    "    \"\\n\",\n",
    "    \"For this part you'll find the 12 patches in the test-set that activate each of the first 5 filters that maximise the activation for that filter.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": false\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"H =  sess.run(h_conv1, feed_dict={x: mnist.test.images})\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add code to visualise patches in the test set that find the most result in \\n\",\n",
    "    \"# the highest activations for filters 0, ... 4\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"anaconda-cloud\": {},\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python [conda root]\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"conda-root-py\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 2\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython2\",\n",
    "   \"version\": \"2.7.12\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 1\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
